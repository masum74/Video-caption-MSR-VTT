{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, encoding_size:int):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(VideoEncoder, self).__init__()\n",
    "        \n",
    "        self.base_network = models.resnet152(pretrained = True)\n",
    "        self.base_network.fc = nn.Linear(self.base_network.fc.in_features, encoding_size)\n",
    "        self.bn = nn.BatchNorm1d(encoding_size, momentum=0.01)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \n",
    "        self.base_network.fc.weight.data.normal_(0.0, 0.02)\n",
    "        self.base_network.fc.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        for t in range(x_3d.size(1)):\n",
    "            # ResNet CNN\n",
    "            with torch.no_grad():\n",
    "                image = x_3d[:, t, :, :, :]\n",
    "                x = self.base_network.conv1(image)  # ResNet\n",
    "                x = self.base_network.bn1(x)\n",
    "                x = self.base_network.relu(x)\n",
    "                x = self.base_network.maxpool(x)\n",
    "\n",
    "                x = self.base_network.layer1(x)\n",
    "                x = self.base_network.layer2(x)\n",
    "                x = self.base_network.layer3(x)\n",
    "                x = self.base_network.layer4(x)\n",
    "\n",
    "                x = self.base_network.avgpool(x)\n",
    "                x = torch.flatten(x, 1) \n",
    "\n",
    "            featureMap = self.base_network.fc(x)\n",
    "            featureMap = self.bn(featureMap)\n",
    "            cnn_embed_seq.append(featureMap)\n",
    "\n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "\n",
    "#         m = nn.MaxPool2d(2, stride=1)\n",
    "#         cnn_embed_seq = m(cnn_embed_seq)\n",
    "        \n",
    "        return torch.flatten(cnn_embed_seq,1)\n",
    "        #return cnn_embed_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
