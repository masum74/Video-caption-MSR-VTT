{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import pytorch_lightning as pl\n",
    "import albumentations as alb\n",
    "\n",
    "%run VideoEncoder.ipynb\n",
    "%run TextDecoder.ipynb\n",
    "\n",
    "class VideoCaptioner(pl.LightningModule):\n",
    "    def __init__(self, textTokenizer, embedding_size: int, state_size: int, encoding_size: int, val_data = None):\n",
    "        super(VideoCaptioner, self).__init__()\n",
    "        self.vocabulary_size = len(textTokenizer.vocab)\n",
    "        self.padding_token_id = textTokenizer.vocab.stoi[\"<pad>\"]\n",
    "        \n",
    "        self.val_data = val_data\n",
    "        \n",
    "        self.video_encoder = VideoEncoder(encoding_size = encoding_size)\n",
    "        self.text_decoder = TextDecoder(embedding_size, \n",
    "                                        state_size, \n",
    "                                        self.vocabulary_size)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(\n",
    "            ignore_index = self.padding_token_id)\n",
    "        \n",
    "        self.text_tokenizer = textTokenizer\n",
    "\n",
    "        self.video_encoder_learning_rate = 1e-3\n",
    "        self.text_decoder_learning_rate = 1e-3\n",
    "\n",
    "    def forward(self, video, text, lengths, teacher_forcing = 1.0):\n",
    "        predicted_scores = list()\n",
    "        encoded_video = self.video_encoder(video)\n",
    "\n",
    "        start_token = text[:, 0]  # This should be the <start> symbol.\n",
    "        \n",
    "        token_scores, state = self.text_decoder((encoded_video,encoded_video), start_token)\n",
    "        predicted_scores.append(token_scores)\n",
    "\n",
    "        for i in range(0, max(lengths) - 2):\n",
    "            if random.random() < teacher_forcing:\n",
    "                current_token = text[:, i + 1]\n",
    "            else:\n",
    "                _, max_token = token_scores.max(dim = 1)\n",
    "                current_token = max_token.detach() # No backprop.\n",
    "            token_scores, state = self.text_decoder(state, current_token)\n",
    "            predicted_scores.append(token_scores)\n",
    "            \n",
    "        # torch.stack(,1) forces batch_first = True on this output.\n",
    "        return torch.stack(predicted_scores, 1), lengths\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        videos, texts, lengths = batch\n",
    "\n",
    "        # Compute the predicted texts.\n",
    "        predicted_texts, _ = self(videos, texts, lengths, \n",
    "                                  teacher_forcing = 1.0)\n",
    "        \n",
    "        # Define the target texts. \n",
    "        # We have to predict everything except the <start> token.\n",
    "        target_texts =  texts[:, 1:].contiguous()\n",
    "\n",
    "        # Use cross entropy loss.\n",
    "        loss = self.criterion(predicted_texts.view(-1, self.vocabulary_size),\n",
    "                              target_texts.view(-1))\n",
    "        self.log('train_loss', loss, on_epoch = True)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        videos, texts, lengths = batch\n",
    "\n",
    "        predicted_texts, _ = self(videos, texts, lengths,\n",
    "                                  teacher_forcing = 0.0)\n",
    "        \n",
    "        target_texts = texts[:, 1:].contiguous()\n",
    "\n",
    "        loss = self.criterion(predicted_texts.view(-1, self.vocabulary_size),\n",
    "                              target_texts.view(-1))\n",
    "        self.log('val_loss', loss, on_epoch = True)\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        print('Validation loss %.2f' %  loss_mean)\n",
    "        \n",
    "        return {'val_loss': loss_mean}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        \n",
    "        loss_mean = torch.stack([x['loss'] for x in outputs[0]]).mean()\n",
    "        print('Training loss %.2f' %  loss_mean)\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return [torch.optim.SGD(list(self.video_encoder.fc1.parameters())+\\\n",
    "#                                 list(self.video_encoder.bn1.parameters())+\\\n",
    "#                                 list(self.video_encoder.fc2.parameters())+\\\n",
    "#                                 list(self.video_encoder.bn2.parameters())+\\\n",
    "#                                 list(self.video_encoder.fc3.parameters()),\n",
    "#                                 lr = self.video_encoder_learning_rate), \\\n",
    "#                 torch.optim.Adam(self.text_decoder.parameters(), \n",
    "#                                  lr = self.text_decoder_learning_rate)], []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [torch.optim.SGD(list(self.video_encoder.base_network.fc.parameters())+\\\n",
    "                                list(self.video_encoder.bn.parameters()),\n",
    "                                lr = self.video_encoder_learning_rate), \\\n",
    "                torch.optim.Adam(self.text_decoder.parameters(),\n",
    "                                 lr = self.text_decoder_learning_rate)], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
