{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas, os, pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DatasetManager(Dataset):\n",
    "    def __init__(self, tokenizer, \n",
    "                 split: str,\n",
    "                 json_file_path: str, # location of the json file\n",
    "                 video_file_path: str, # location of the video files\n",
    "                 vocabulary_size: int,\n",
    "                 frames: int,\n",
    "                 video_transform = None):\n",
    "        \n",
    "        assert os.path.exists(json_file_path), json_file_path\n",
    "        assert os.path.exists(video_file_path), video_file_path\n",
    "        assert vocabulary_size > 0\n",
    "        \n",
    "        self.video_file_path = video_file_path\n",
    "        self.textTokenizer = tokenizer\n",
    "        self.video_transform = video_transform\n",
    "        self.frames = frames\n",
    "        self.split = split\n",
    "        \n",
    "        with open(json_file_path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        videoInfo = json_data['sentences']\n",
    "        \n",
    "        random.shuffle(videoInfo)\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            splittedVideoInfo = videoInfo[:20000]\n",
    "\n",
    "        elif self.split == \"validation\":  \n",
    "            splittedVideoInfo = videoInfo[20000:25000]\n",
    "\n",
    "        self.videos = [idx['video_id'] for idx in splittedVideoInfo]\n",
    "        captions = [idx['caption'] for idx in splittedVideoInfo]\n",
    "        \n",
    "        self.texts = list()\n",
    "\n",
    "        for caption in(captions):\n",
    "            tokenized_caption = tokenizer.preprocess(caption)\n",
    "            self.texts.append(tokenized_caption)\n",
    "                \n",
    "        if self.split == \"train\" and not hasattr(tokenizer, \"vocab\"):\n",
    "            self.buildVocab(videoInfo, vocabulary_size)\n",
    "    \n",
    "        # Numericalize all texts.\n",
    "        for i in tqdm(range(0, len(self.texts))):\n",
    "                    self.texts[i] = tokenizer.process([self.texts[i]])\n",
    "\n",
    "    def buildVocab(self, videoInfo, vocabulary_size):\n",
    "        texts = list()\n",
    "        captions = [idx['caption'] for idx in videoInfo]\n",
    "\n",
    "        for caption in(captions):\n",
    "            tokenized_caption = self.textTokenizer.preprocess(caption)\n",
    "            texts.append(tokenized_caption)\n",
    "\n",
    "        self.textTokenizer.build_vocab(texts, max_size = vocabulary_size)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        vidFile = cv2.VideoCapture(self.video_file_path+self.videos[i]+'.mp4')\n",
    "        \n",
    "        if (vidFile.isOpened() == False):\n",
    "            path = self.video_file_path+self.videos[i]\n",
    "            print('Error while trying to read video ', path,'. Please check path again')\n",
    "\n",
    "        clips = [] \n",
    "        \n",
    "        while(vidFile.isOpened()):\n",
    "            ret, frame = vidFile.read()\n",
    "            if ret == True:\n",
    "                image = frame.copy()\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = self.video_transform(image=frame)['image']\n",
    "                clips.append(frame)\n",
    "                if len(clips) == self.frames:\n",
    "                    with torch.no_grad(): \n",
    "                        input_frames = np.array(clips)       \n",
    "                        #input_frames = np.transpose(input_frames, (3, 0, 1, 2))\n",
    "                        input_frames = np.transpose(input_frames, (0, 3, 1, 2))\n",
    "                        input_frames = torch.tensor(input_frames, dtype=torch.float32)\n",
    "\n",
    "                        return input_frames, self.texts[i].squeeze()\n",
    "    \n",
    "    \n",
    "    # To be used in the Data Loader collate_fn parameter.\n",
    "    def create_batch(self, batch):\n",
    "        videos, texts = zip(*batch)\n",
    "\n",
    "        # Compute text lengths for Pytorch's RNN library.\n",
    "        text_lengths = [len(text) for text in texts]\n",
    "\n",
    "        # Stack videos and pad text.\n",
    "        stacked_videos = torch.stack(videos)\n",
    "        padded_texts = pad_sequence(texts, batch_first = self.textTokenizer.batch_first, \n",
    "                                    padding_value = self.textTokenizer.vocab.stoi[\"<pad>\"])\n",
    "\n",
    "        return stacked_videos, padded_texts, text_lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
